{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LZ78 Sequential Probability Assignment: Python Interface for Rust Implementation\n",
    "This code is associated with the paper [A Family of LZ78-based Universal Sequential Probability Assignments](https://arxiv.org/abs/2410.06589).\n",
    "\n",
    "The codebase is in Rust, with Python bindings. This tutorial goes through how to use the Python API; if you are familiar with Rust or want to learn, feel free to look at `crates/lz78` for the source code and `crates/python` for the bindings (the former is well-documented, whereas the latter is not-so-well-documented)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "You need to install Rust and Maturin, and then install the Python bindings for the `lz78` library as an editable Python package.\n",
    "1. Install Rust: [Instructions](https://www.rust-lang.org/tools/install).\n",
    "    - After installing Rust, close and reopen your terminal before proceeding.\n",
    "2. If applicable, switch to the desired Python environment.\n",
    "3. Install Maturin: `pip install maturin`\n",
    "4. Install the `lz78` Python package: `cd crates/python && maturin develop && cd ../..`\n",
    "\n",
    "You also need the `lorem` ([context](https://loremipsum.io/)), `numpy`, and `requests` Python packages for this tutorial.\n",
    "\n",
    "### Setup Notes\n",
    "If you are modifying the Rust code and are using VSCode, you have to do a few more steps:\n",
    "1. Install the `rust` and `rust-analyzer` extensions.\n",
    "2. Adding extra environment variablers to the rust server:\n",
    "    - In a terminal, run `echo $PATH`, and copy the output.\n",
    "    - Go to `Preferences: Remote Settings (JSON)` if you are working on a remote machine, or `Preferences: User Settings (JSON)` if you are working locally (you can find this by pressing `F1` and then searching), and make sure it looks like the following:\n",
    "        ```\n",
    "        {\n",
    "            \"rust-analyzer.runnables.extraEnv\": {\n",
    "                \"PATH\": \"<the string you copied in the previous step>\"\n",
    "            },\n",
    "        }\n",
    "        ```\n",
    "3. Open `User Settings (JSON)` and add `\"editor.formatOnSave\": true`\n",
    "4. Restart your VSCode window.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lz78 import Sequence, LZ78Encoder, CharacterMap, BlockLZ78Encoder, LZ78SPA\n",
    "from lz78 import encoded_sequence_from_bytes, spa_from_bytes\n",
    "import numpy as np\n",
    "import lorem\n",
    "import requests\n",
    "from sys import stdout\n",
    "from os import makedirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Sometimes, Jupyter doesn't register that a cell containing code from the `lz78` library has started running, so it seems like the cell is waiting to run until it finishes. This can be annoying for operations that take a while to run, and **can be remedied by putting `stdout.flush()` at the beginning of the cell**.\n",
    "- For a description of all classes and functions, got to `crates/python/lz78.pyi`. The docstrings there are the same as the ones that appear when you hover over a class or method in Jupyter/most IDEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequences\n",
    "\n",
    "Any sequence of data that can be LZ78-encoded (i.e., a list of integers or a String) is represented as a `Sequence` object.\n",
    "Storing sequences as this object (as opposed to raw lists or strings) allows for a common interface that streamlines the LZ78 encoding process.\n",
    "\n",
    "Each sequence is associated with an alphabet size, A.\n",
    "\n",
    "If the sequence consists of integers, they must be in the range ${0, 1, ..., A-1}$.\n",
    "If $A < 256$, the sequence is stored internally as bytes.\n",
    "Otherwise, it is stored as `uint32`.\n",
    "\n",
    "If the sequence is a string, a `CharacterMap` object maps each character to a number between 0 and A-1.\n",
    "More on this later.\n",
    "\n",
    "**Inputs**:\n",
    "- data: either a list of integers or a string.\n",
    "- alphabet_size (optional): the size of the alphabet.\n",
    "    If this is `None`, then the alphabet size is inferred from the data.\n",
    "- charmap (optional): A `CharacterMap` object; only valid if `data` is a string.\n",
    "    If `data` is a string and this is `None`, then the character map is inferred from the data.\n",
    "\n",
    "The methods available for a `Sequence` object are described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Example: Integer Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randint(0, 2, size=(10_000_000,))\n",
    "int_sequence = Sequence(data, alphabet_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A limited number of Python list operations work on `Sequence`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n",
      "[0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(int_sequence))\n",
    "print(int_sequence[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a note, indexing a string-based sequence in this manner will return the integer-based representation of the string and not the string itself. You will have to use the corresponding character map to map these integers back to a string representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `extend`\n",
    "\n",
    "Adds data to the end of the sequence.\n",
    "Data must be over the same alphabet as the current sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_data = np.random.randint(0, 2, size=(200,))\n",
    "int_sequence.extend(more_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `alphabet_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_sequence.alphabet_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `get_data`\n",
    "Returns the full sequence as an integer list or string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "extracted_data = int_sequence.get_data()\n",
    "print(type(extracted_data))\n",
    "print(extracted_data[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 `CharacterMap`\n",
    "A sequence is defined as integers from 0 to A-1, where A is the alphabet size, so we need a way to map strings to such integer-based sequences.\n",
    "\n",
    "The `CharacterMap` class maps characters in a string to integer values in a contiguous range, so that a string can be used as an individual sequence.\n",
    "It has the capability to **encode** a string into the corresponding integer representation, and **decode** a list of integers into a string.\n",
    "\n",
    "Inputs:\n",
    "- data: a string consisting of all of the characters that will appear in the character map. For instance, a common use case is:\n",
    "    ```\n",
    "    charmap = CharacterMap(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some dummy data and make a character map\n",
    "s = \" \".join(([lorem.paragraph() for _ in range(10)]))\n",
    "charmap = CharacterMap(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `encode`\n",
    "Takes a string and returns the corresponding integer representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 7, 17, 11, 4, 5, 13, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charmap.encode(\"lorem ipsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It errors if any characters to be encoded are not in the alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Character \"h\" not in mapping",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# this should error, but with a helpful warning message\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m charmap\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello world\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Character \"h\" not in mapping"
     ]
    }
   ],
   "source": [
    "# this should error, but with a helpful warning message!\n",
    "charmap.encode(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `filter_string`\n",
    "Takes a string and removes any characters that are not present in the character mapping.\n",
    "This is useful if you have some text with special characters, and you don't want the special characters to be in the alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ello orld. Lorem ipsum '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charmap.filter_string(\"hello world. Lorem ipsum! @#$%^&*()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `decode`\n",
    "Decodes an integer representation of a string into the string itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lorem ipsum'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charmap.decode(charmap.encode(\"lorem ipsum\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `alphabet_size`\n",
    "Returns how many characters can be represented by the character mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charmap.alphabet_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Example: Character Sequence\n",
    "A string-based sequence is sometimes referred to as a character sequence. It has the same interface as an integer sequence, except there is an underlying `CharacterMap` object that maps characters to corresponding integer values within the alphabet.\n",
    "\n",
    "You can pass in a `CharacterMap` upon instantiation, or else the character map will be inferred from the data.\n",
    "\n",
    "**Note**: if you pass in a `CharacterMap`, and the input string has characters not present in the character map, instantiation will error.\n",
    "To avoid this, you can use `CharacterMap.filter` beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout.flush()\n",
    "charmap = CharacterMap(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ. ?,\")\n",
    "s = \" \".join(([lorem.paragraph() for _ in range(1000)]))\n",
    "charseq = Sequence(s, charmap=charmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing a character sequence returns the integer representations of the corresponding characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 0, 53, 11, 0, 1, 14, 17, 4, 53, 13, 4, 16, 20, 4, 53, 0, 11, 8, 16, 20, 0, 12, 53, 12, 0, 6, 13, 0, 12]\n"
     ]
    }
   ],
   "source": [
    "print(charseq[100:130])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `get_character_map`\n",
    "Returns the underlying `CharacterMap` object.\n",
    "This will error if the sequence is not a character sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ra labore neque aliquam magnam'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charmap = charseq.get_character_map()\n",
    "charmap.decode(charseq[100:130])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LZ78 Compression\n",
    "The `LZ78Encoder` object performs plain LZ78 encoding and decoding, as described in \"Compression of individual sequences via variable-rate coding\" (Ziv, Lempel 1978)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 `CompressedSequence` object\n",
    "A `CompressedSequence` object stores an encoded bitstream, as well as some auxiliary information needed for decoding.\n",
    "`CompressedSequence` objects cannot be instantiated directly,\n",
    "but rather are returned by `LZ78Encoder.encode`.\n",
    "\n",
    "The main functionality is:\n",
    "1. Getting the compression ratio as `(encoded size) / (uncompressed len * log A)`,\n",
    "    where A is the size of the alphabet.\n",
    "2. Getting a byte array representing this object, so that the compressed\n",
    "    sequence can be stored to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Example: LZ78 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an input sequence to compress\n",
    "stdout.flush()\n",
    "data = \" \".join(([lorem.paragraph() for _ in range(10_000)]))\n",
    "charseq = Sequence(data)\n",
    "encoder = LZ78Encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `LZ78Encoder` Instance method: `encode`\n",
    "Performs LZ78 encoding on an individual sequence, and returns a `CompressedSequence` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout.flush()\n",
    "encoded = encoder.encode(charseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `CompressedSequence` Instance method: `compression_ratio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3189317286014557"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.compression_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a `CompressedSequence` object\n",
    "`CompressedSequence` has functionality to produce a `bytes` object representation, which can be written directly to a file.\n",
    "The function `encoded_sequence_from_bytes` produces a `CompressedSequence` object from this `bytes` representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout.flush()\n",
    "bytes = encoded.to_bytes()\n",
    "\n",
    "makedirs(\"test_data\", exist_ok=True)\n",
    "with open(\"test_data/saved_encoded_sequence.bin\", 'wb') as file:\n",
    "    file.write(bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's read the compressed sequence from the file and decode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_data/saved_encoded_sequence.bin\", 'rb') as file:\n",
    "    encoded_bytes = file.read()\n",
    "encoded = encoded_sequence_from_bytes(encoded_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout.flush()\n",
    "decoded = encoder.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert decoded.get_data() == data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Block-Wise Compression\n",
    "Sometimes, it might be useful to loop through blocks of data and perform LZ78 encoding on each block (e.g., if you need to do data processing before LZ78 compression and want to have some sort of pipeline parallelism).\n",
    "\n",
    "The `BlockLZ78Encoder` has this functionality: you can pass in the input sequence to be compressed in chunks, and the output (`encoder.get_encoded_sequence()`) is as if the full concatenated sequence was passed in to an LZ78 encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "charmap = CharacterMap(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ. ,?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = BlockLZ78Encoder(charmap.alphabet_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `encode_block`\n",
    "Encodes a block using LZ78, starting at the end of the previous block.\n",
    "\n",
    "All blocks must be over the same alphabet, or else the call to `encode_block` will error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout.flush()\n",
    "for _ in range(1000):\n",
    "    encoder.encode_block(Sequence(lorem.paragraph(), charmap=charmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected String Sequence with character mapping ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '.', ' ', ',', '?'], got Byte (U8) Sequence with alphabet size 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Oops, this won't work!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m encoder\u001b[38;5;241m.\u001b[39mencode_block(Sequence([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m10\u001b[39m]))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected String Sequence with character mapping ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '.', ' ', ',', '?'], got Byte (U8) Sequence with alphabet size 11"
     ]
    }
   ],
   "source": [
    "# Oops, this won't work!\n",
    "encoder.encode_block(Sequence([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `get_encoded_sequence`\n",
    "Returns the compressed sequence, which is equivalent to the output of `LZ78Encoder.encode` on the concatenation of all inputs to `encode_block` thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3503199517726898"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sequence = encoder.get_encoded_sequence()\n",
    "encoded_sequence.compression_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `decode`\n",
    "Decompresses the compressed sequence that has been constructed thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 15, 19, 0, 19, 4, 12, 53, 21, 4, 11, 8, 19, 53, 13, 14, 13, 53, 12, 0, 6, 13, 0, 12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'uptatem velit non magnam'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdout.flush()\n",
    "decoded = encoder.decode()\n",
    "print(decoded[376:400])\n",
    "charmap.decode(decoded[376:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LZ78 Sequential Probability Assignment (SPA)\n",
    "The `LZ78SPA` class is the implementation of the family of sequential probability assignments discussed in [A Family of LZ78-based Universal Sequential Probability Assignments](https://arxiv.org/abs/2410.06589), for Dirichelt priors.\n",
    "In this section, `gamma` refers to the Dirichlet parameter.\n",
    "\n",
    "Under this prior, the sequential probability assignment is an additive\n",
    "perturbation of the emprical distribution, conditioned on the LZ78 prefix\n",
    "of each symbol (i.e., the probability model is proportional to the\n",
    "number of times each node of the LZ78 tree has been visited, plus gamma).\n",
    "\n",
    "This SPA has the following capabilities:\n",
    "- training on one or more sequences,\n",
    "- log loss (\"perplexity\") computation for test sequences,\n",
    "- SPA computation (using the LZ78 context reached at the end of parsing\n",
    "    the last training block),\n",
    "- sequence generation.\n",
    "\n",
    "Note that the LZ78SPA does not perform compression; you would have to use\n",
    "a separate BlockLZ78Encoder object to perform block-wise compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Example: LZ78 SPA on Markov Data\n",
    "\n",
    "We will use the Markov probability source used in [(Rajaraman et al, 2024)](https://arxiv.org/pdf/2404.08335), where the transition probability depends solely on $x_{t-k}$.\n",
    "Specifically, $x_t = x_{t-k}$ with probability $0.9$, and otherwise $x_t$ is picked uniformly at random from the rest of the alphabet.\n",
    "\n",
    "The SPA works best when the alphabet size is $2$, but you can try out other alphabet sizes too.\n",
    "\n",
    "First, we define some helper functions for generating the data (don't worry about understanding these; they are irrelevant to understanding the SPA itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods for generating data; feel free run the cell without\n",
    "# reading the code\n",
    "def sample_index_from_dist(probabilities):\n",
    "    cdf = np.cumsum(probabilities)\n",
    "    cdf[-1] = 1 # in case of FP error\n",
    "    return int(np.where(np.random.random() < cdf)[0][0])\n",
    "\n",
    "def entropy(probs):\n",
    "    return sum([-x * np.log2(x) for x in probs if x > 0])\n",
    "\n",
    "def get_stationary_dist(transition_probabilities):\n",
    "    eigvals, eigvecs = np.linalg.eig(transition_probabilities.T)\n",
    "    # all eigenvalues will be <= 1, and one will be =1\n",
    "    stationary_dist = eigvecs[:, np.argmax(eigvals)]\n",
    "    return stationary_dist / sum(stationary_dist)\n",
    "\n",
    "def entropy_rate(transition_probabilities):\n",
    "    stationary_dist = get_stationary_dist(transition_probabilities)\n",
    "    return sum([prob * entropy(transition_probabilities[i]) \n",
    "                for i, prob in enumerate(stationary_dist)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some data to pass through the SPA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can change these\n",
    "ALPHABET_SIZE = 2\n",
    "PEAK_PROB = 0.9\n",
    "K = 5\n",
    "N = 1_000_000\n",
    "N_TEST = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data array; feel free to ignore this code and just run the cell\n",
    "transition_probabilities = np.eye(ALPHABET_SIZE) * PEAK_PROB + \\\n",
    "    (np.ones((ALPHABET_SIZE, ALPHABET_SIZE)) - np.eye(ALPHABET_SIZE)) * (1 - PEAK_PROB) / (ALPHABET_SIZE - 1)\n",
    "start_prob = np.ones(ALPHABET_SIZE) / ALPHABET_SIZE\n",
    "\n",
    "data = np.zeros(N, dtype=int)\n",
    "for i in range(K):\n",
    "    data[i] = sample_index_from_dist(start_prob)\n",
    "for i in range(K,N):\n",
    "    data[i] = sample_index_from_dist(transition_probabilities[data[i-K]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = Sequence(data[:-N_TEST])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `train_on_block`\n",
    "\n",
    "Use a block of data to update the SPA. If `include_prev_context` is\n",
    "true, then this block is considered to be from the same sequence as\n",
    "the previous. Otherwise, it is assumed to be a separate sequence, and\n",
    "we return to the root of the LZ78 prefix tree.\n",
    "\n",
    "It returns the self-entropy log loss incurred while processing this\n",
    "sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa = LZ78SPA(ALPHABET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6277613794856698"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdout.flush()\n",
    "spa.train_on_block(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `compute_test_loss`\n",
    "After training a SPA, you can compute the log loss of a test sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7026905466910377"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdout.flush()\n",
    "spa.compute_test_loss(Sequence(data[-N_TEST:]), include_prev_context=True) / N_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `get_normalized_log_loss`\n",
    "Gets the normaliized self-entropy log loss incurred from training the SPA thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6277613794856698"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spa.get_normalized_log_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `compute_spa_at_current_state`\n",
    "Computes the SPA for every symbol in the alphabet, using the LZ78 context reached at the end of parsing the last training block.\n",
    "\n",
    "In this case, the method will return a two-element list, where the first element is the estimated probability that the next symbol is $0$ and the second is the estimated probability that the next symbol is $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8855421686746988, 0.1144578313253012]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One component \"should\" be 0.9 and the other \"should\" be 0.1, but this is\n",
    "# not necessarily the case. e.g., if we are at the top of the LZ78 prefix tree\n",
    "# or at a leaf, we can expect the SPA to be closer to [0.5, 0.5]\n",
    "spa.compute_spa_at_current_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `to_bytes`\n",
    "This works the same as the corresponding index of `CompressedSequence`; refer to the LZ78 Encoding part of the tutorial for more details.\n",
    "\n",
    "The method `spa_from_bytes` reconstructs a SPA from its `bytes` representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Example: Text Generation\n",
    "\n",
    "Let's use the LZ78 SPA to generate some text based on Sherlock Holmes novels.\n",
    "\n",
    "This requires the `requests` library and an internet connection.\n",
    "If you don't have either, you can perform the same experiment any text you'd like, including the lorem ipsum text from the beginning of this tutorial.\n",
    "Just make sure you have enough training data (e.g., the Sherlock novel used for this example is 500 kB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = requests.get(\"https://www.gutenberg.org/cache/epub/1661/pg1661.txt\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our own character map and filter the text based on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout.flush()\n",
    "charmap = CharacterMap(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ. ,?\\n\\\"';:\\t-_\")\n",
    "filtered_text = charmap.filter_string(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the SPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7578532624402827"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdout.flush()\n",
    "spa = LZ78SPA(charmap.alphabet_size(), gamma=0.2)\n",
    "spa.train_on_block(Sequence(filtered_text, charmap=charmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `generate_data`\n",
    "Generates a sequence of data, using temperature and top-k sampling (see\n",
    "the \"Experiments\" section of [Sagan and Weissman 2024] for more details).\n",
    "\n",
    "Inputs:\n",
    "- **len**: number of symbols to generate\n",
    "- **min_context**: the SPA tries to maintain a context of at least a\n",
    "    certain length at all times. So, when we reach a leaf of the LZ78\n",
    "    prefix tree, we try traversing the tree with different suffixes of\n",
    "    the generated sequence until we get a sufficiently long context\n",
    "    for the next symbol.\n",
    "- **temperature**: a measure of how \"random\" the generated sequence is. A\n",
    "    temperature of 0 deterministically generates the most likely\n",
    "    symbols, and a temperature of 1 samples directly from the SPA.\n",
    "    Temperature values around 0.1 or 0.2 function well.\n",
    "- **top_k**: forces the generated symbols to be of the top_k most likely\n",
    "    symbols at each timestep.\n",
    "- **seed_data**: you can specify that the sequence of generated data\n",
    "be the continuation of the specified sequence.\n",
    "\n",
    "Returns a tuple of the generated sequence and that sequence's log loss,\n",
    "or perplexity.\n",
    "\n",
    "Errors if the SPA has not been trained so far, or if the seed data is\n",
    "not over the same alphabet as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "(generated, loss) = spa.generate_data(\n",
    "    500,\n",
    "    min_context=5,\n",
    "    temperature=0.1,\n",
    "    top_k=5,\n",
    "    seed_data=Sequence(\"This \", charmap=charmap)\n",
    ")\n",
    "generated = generated.get_data()\n",
    "generated = \"This \" + generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the generated text will not appear very high-quality for a few reasons:\n",
    "1. We don't have a lot of training data. LZ78-based algorithms generally take a lot of data to train; empirically, the SPA does better with many megabytes of data, at least.\n",
    "2. The `min_context` parameter is being used very heuristically, and could potentially be forcing us to the bottom of the LZ78 tree, where we don't have a lot of data, making the SPA closer to uniform. If you see long gibberish strings, try decreasing `min_context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is money father which had been out of the coronet into the advertisedm heir\n",
      "a cotnsi a cloise to the door opf oened away to the stablise of the street. Suim\n",
      "oddnsedt f the straini thng to him, and the chiarg we a rsoepnal tdo he stone in\n",
      " the conversa to\n",
      "imthe sightnal there was a very sharpl iexn this morning I am a\n",
      "lfll that the contrasnge for me to the contrary, fsaor the coronet into the adve\n",
      "rtisedm heira cotmmchent. I saw himself interest in the colour?l own little tro \n",
      "kng pow what is not be ua\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(generated), 80):\n",
    "    print(generated[i:i+80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
